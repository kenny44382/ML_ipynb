{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ancient-gospel",
   "metadata": {},
   "source": [
    "# __Convolutional Neural Network__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-poker",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thorough-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bronze-hollywood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-belle",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-specification",
   "metadata": {},
   "source": [
    "### _Preprocessing the Training Set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "configured-royal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generator with Image Augmentation (in order to reduce overfitting of image feature data)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                target_size=(64,64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-marketplace",
   "metadata": {},
   "source": [
    "### _Preprocessing the Test Set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emotional-nickname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Apply only feature scaling to the Test Set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                           target_size=(64,64),\n",
    "                                           batch_size=32,\n",
    "                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-nowhere",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-cream",
   "metadata": {},
   "source": [
    "### _Initialising the CNN_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "authorized-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-parliament",
   "metadata": {},
   "source": [
    "### _Step 1 - Convolution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "active-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-accident",
   "metadata": {},
   "source": [
    "### _Step 2 - Pooling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accepted-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-southwest",
   "metadata": {},
   "source": [
    "### _Adding a second convolutional layer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baking-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-bargain",
   "metadata": {},
   "source": [
    "### _Step 3 - Flattening_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "neutral-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-template",
   "metadata": {},
   "source": [
    "### _Step 4 - Full Connection_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imperial-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-ozone",
   "metadata": {},
   "source": [
    "### _Step 5 - Output Layer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nasty-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-jefferson",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-piano",
   "metadata": {},
   "source": [
    "### _Compiling the CNN_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incorrect-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-congo",
   "metadata": {},
   "source": [
    "### _Training the CNN on the training set and evaluating it on the Test set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "russian-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 238s 951ms/step - loss: 0.6733 - accuracy: 0.5788 - val_loss: 0.6213 - val_accuracy: 0.6760\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 94s 376ms/step - loss: 0.6173 - accuracy: 0.6629 - val_loss: 0.5791 - val_accuracy: 0.6860\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 111s 446ms/step - loss: 0.5670 - accuracy: 0.7116 - val_loss: 0.5408 - val_accuracy: 0.7340\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 103s 411ms/step - loss: 0.5296 - accuracy: 0.7358 - val_loss: 0.4999 - val_accuracy: 0.7650\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 94s 377ms/step - loss: 0.4998 - accuracy: 0.7534 - val_loss: 0.4859 - val_accuracy: 0.7770\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 103s 411ms/step - loss: 0.4798 - accuracy: 0.7697 - val_loss: 0.4945 - val_accuracy: 0.7670\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.4625 - accuracy: 0.7781 - val_loss: 0.4878 - val_accuracy: 0.7780\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 118s 473ms/step - loss: 0.4533 - accuracy: 0.7871 - val_loss: 0.4822 - val_accuracy: 0.7690\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 113s 453ms/step - loss: 0.4381 - accuracy: 0.7928 - val_loss: 0.4713 - val_accuracy: 0.7775\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 110s 438ms/step - loss: 0.4234 - accuracy: 0.8030 - val_loss: 0.4689 - val_accuracy: 0.7835\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 0.3996 - accuracy: 0.8159 - val_loss: 0.4750 - val_accuracy: 0.7945\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 146s 584ms/step - loss: 0.3967 - accuracy: 0.8171 - val_loss: 0.5323 - val_accuracy: 0.7660\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 152s 606ms/step - loss: 0.3848 - accuracy: 0.8290 - val_loss: 0.4734 - val_accuracy: 0.7915\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 153s 612ms/step - loss: 0.3666 - accuracy: 0.8303 - val_loss: 0.4552 - val_accuracy: 0.7965\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 176s 703ms/step - loss: 0.3484 - accuracy: 0.8431 - val_loss: 0.5016 - val_accuracy: 0.7855\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 151s 603ms/step - loss: 0.3408 - accuracy: 0.8487 - val_loss: 0.4691 - val_accuracy: 0.7925\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.3314 - accuracy: 0.8525 - val_loss: 0.4798 - val_accuracy: 0.8005\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 154s 617ms/step - loss: 0.3140 - accuracy: 0.8601 - val_loss: 0.4736 - val_accuracy: 0.8035\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 156s 626ms/step - loss: 0.3097 - accuracy: 0.8633 - val_loss: 0.5159 - val_accuracy: 0.7950\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 176s 704ms/step - loss: 0.2964 - accuracy: 0.8717 - val_loss: 0.5036 - val_accuracy: 0.7955\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 167s 670ms/step - loss: 0.2880 - accuracy: 0.8766 - val_loss: 0.5066 - val_accuracy: 0.7825\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 172s 690ms/step - loss: 0.2671 - accuracy: 0.8869 - val_loss: 0.5421 - val_accuracy: 0.7955\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 175s 701ms/step - loss: 0.2582 - accuracy: 0.8935 - val_loss: 0.5489 - val_accuracy: 0.7885\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 170s 679ms/step - loss: 0.2472 - accuracy: 0.8941 - val_loss: 0.5037 - val_accuracy: 0.8090\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 171s 685ms/step - loss: 0.2397 - accuracy: 0.9019 - val_loss: 0.5423 - val_accuracy: 0.7960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25380f072b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-architecture",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alternative-desperate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "print(training_set.class_indices)\n",
    "if result[0][0]== 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "superior-antigua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "certain-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "print(training_set.class_indices)\n",
    "if result[0][0]== 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "industrial-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowEnv",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
